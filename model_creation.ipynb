{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "\n",
    "In this jupyter notebook we shall look at taking the preprocessed data  generated by preprocessing_part_2.ipynb and creating machine learning model from it \n",
    "that reads each review and tries to predict what its average score is. Thus we are building a text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with the relevant imports\n",
    "\n",
    "#use to visualise the data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#used to build the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: load and inspect the csv with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first load the data with pandas\n",
    "df=pd.read_csv(\"./data/data_ready_for_model.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>moved uk end august got virgin media broadband...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>truly attrocious service terms broadband custo...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>hard cancel contract. phone 2 hours t o spend ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>pay 350mbps package managed 250mbps upload 34 ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>worst customer service: -the bots ask irreleva...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Comments  \\\n",
       "0           0  moved uk end august got virgin media broadband...   \n",
       "1           1  truly attrocious service terms broadband custo...   \n",
       "2           2  hard cancel contract. phone 2 hours t o spend ...   \n",
       "3           3  pay 350mbps package managed 250mbps upload 34 ...   \n",
       "4           4  worst customer service: -the bots ask irreleva...   \n",
       "\n",
       "   Average Score  \n",
       "0            1.0  \n",
       "1            1.0  \n",
       "2            2.0  \n",
       "3            2.0  \n",
       "4            2.0  "
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True) #unneeded column, resulted when csv was created from dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step before splitting our data into train test split sets is to tokenize the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max words to be used.\n",
    "max_words=5000 \n",
    "#max no of words per complaint:\n",
    "max_sequence=250\n",
    "#fixed\n",
    "embedding_dim=250\n",
    "\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df[\"Comments\"].values)\n",
    "word_index=tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truncate and pad the input sequences so that they are all in the same length for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 13038 unique tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"found {len(word_index)} unique tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (4342, 250)\n"
     ]
    }
   ],
   "source": [
    "X = tokenizer.texts_to_sequences(df['Comments'].values)\n",
    "X = tf.keras.utils.pad_sequences(X, maxlen=max_sequence)\n",
    "print('Shape of data tensor:', X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 prepare the data into train val test sets (code is borrowed from my Wine reviews classification Neural Network). We want our target ot be our \"average score\" and our features to be the \"comments\". We have quite the imbalanced dataset,  because we have more average scores with a score of 1 and two than any other score. Because we are implementing a classification model, this could be especially problematic.\n",
    "\n",
    "To overcome this data we will _stratify_ the data. This is to ensure that relative class frequencies is approximately preserved in each train and validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[\"Average Score\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.340, random_state=0, stratify=y)\n",
    "#60 training, 20 validation, 20 testing\n",
    "X_val, X_test, y_val, y_test =train_test_split(X_temp, y_temp, test_size = 0.5, random_state=0, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the time for creating a neural network has finally arrived! First, let's encode our comments using a text vectorizor model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check our vocabulary. These are just some of the words that have been encoded into vectors: (UNK) represents any unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Embedding(max_words, embedding_dim, input_length=X.shape[1]),#mask=0 so we can handle inputs of variable lengths\n",
    "        #now we have a vector of numbers a nn can comprehend\n",
    "        tf.keras.layers.SpatialDropout1D(0.2),\n",
    "        tf.keras.layers.LSTM(32),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dropout(0.4),\n",
    "        Dense(5, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint(filepath='saved_model', monitor='val_loss', save_best_only=True)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(RMSprop(learning_rate=0.001), \n",
    "             loss = SparseCategoricalCrossentropy(), #categorical cross entropy as multi classification problem\n",
    "                metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 4s 30ms/step - loss: 1.6122 - sparse_categorical_accuracy: 0.0967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.612160325050354, 0.09668412059545517]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_train, y_train) #evaluate performance of model without training it first\n",
    "#accuracy is around 0.36.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "90/90 [==============================] - ETA: 0s - loss: 1.0239 - sparse_categorical_accuracy: 0.5986"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90/90 [==============================] - 18s 166ms/step - loss: 1.0239 - sparse_categorical_accuracy: 0.5986 - val_loss: 0.8599 - val_sparse_categorical_accuracy: 0.6491\n",
      "Epoch 2/50\n",
      "90/90 [==============================] - 12s 138ms/step - loss: 0.8870 - sparse_categorical_accuracy: 0.6370 - val_loss: 0.8941 - val_sparse_categorical_accuracy: 0.6491\n",
      "Epoch 3/50\n",
      "90/90 [==============================] - 13s 144ms/step - loss: 0.8022 - sparse_categorical_accuracy: 0.6524 - val_loss: 0.8919 - val_sparse_categorical_accuracy: 0.6477\n",
      "Epoch 4/50\n",
      "90/90 [==============================] - 13s 139ms/step - loss: 0.6797 - sparse_categorical_accuracy: 0.7194 - val_loss: 1.0377 - val_sparse_categorical_accuracy: 0.6260\n",
      "Epoch 5/50\n",
      "90/90 [==============================] - 12s 129ms/step - loss: 0.5548 - sparse_categorical_accuracy: 0.7892 - val_loss: 1.1613 - val_sparse_categorical_accuracy: 0.6003\n",
      "Epoch 6/50\n",
      "90/90 [==============================] - 12s 138ms/step - loss: 0.4409 - sparse_categorical_accuracy: 0.8380 - val_loss: 1.2716 - val_sparse_categorical_accuracy: 0.5637\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=50, validation_data=(X_val, y_val), callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/model1\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_model/model1\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"saved_model/model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model has trained but has a val accuracy of only 0.6491. We can see that it is clearly overfitting. So we will have to try and improve it. As our dataset is quite unbalanced, one way to do this is by using Jaccard's similarity metric.\n",
    "\n",
    "Jaccard Similarity is a measure of how similar two sets are based on the items present in both the sets. It is defined as the fraction of number of common elements in two sets to the total number of elements in the union of the two sets. \n",
    "\n",
    "We can use it in the \"columns\" content to find all the comments that are very similar to each other and remove them from the dataframe, essentially removing any \"quasi duplicate\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3b44d0901b27b90b29836124934e882c64ed863e8408c20b31f73c13ca8ef04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
