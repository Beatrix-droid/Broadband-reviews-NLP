{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "\n",
    "In this jupyter notebook we shall look at taking the preprocessed data  generated by preprocessing_part_2.ipynb and creating machine learning model from it \n",
    "that reads each review and tries to predict what its average score is. Thus we are building a text classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start with the relevant imports\n",
    "\n",
    "#use to visualise the data \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#used to build the model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, TextVectorization, Dropout, Embedding, LSTM\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import SparseCategoricalCrossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: load and inspect the csv with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first load the data with pandas\n",
    "df=pd.read_csv(\"./data/data_ready_for_model.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Comments</th>\n",
       "      <th>Average Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Moved back to the UK end of August and got Vir...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>A truly attrocious service, both in terms of b...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>They make it as hard as they can for you to ca...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Pay for the 350Mbps package but only ever mana...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The worst customer service:\\r-The bots ask irr...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           Comments  \\\n",
       "0           0  Moved back to the UK end of August and got Vir...   \n",
       "1           1  A truly attrocious service, both in terms of b...   \n",
       "2           2  They make it as hard as they can for you to ca...   \n",
       "3           3  Pay for the 350Mbps package but only ever mana...   \n",
       "4           4  The worst customer service:\\r-The bots ask irr...   \n",
       "\n",
       "   Average Score  \n",
       "0            1.0  \n",
       "1            1.0  \n",
       "2            2.0  \n",
       "3            2.0  \n",
       "4            2.0  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True) #unneeded column, resulted when csv was created from dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 prepare the data into train val test sets (code is borrowed from my Wine reviews classification Neural Network). We want our target ot be our \"average score\" and our features to be the \"comments\". We have quite the imbalanced dataset,  because we have more average scores with a score of 1 and two than any other score. Because we are implementing a classification model, this could be especially problematic.\n",
    "\n",
    "To overcome this data we will _stratify_ the data. This is to ensure that relative class frequencies is approximately preserved in each train and validation fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.drop(\"Average Score\", axis=1)\n",
    "y=df[\"Average Score\"]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.340, random_state=0, stratify=y)\n",
    "#60 training, 20 validation, 20 testing\n",
    "X_val, X_test, y_val, y_test =train_test_split(X_temp, y_temp, test_size = 0.5, random_state=0, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def df_to_dataset(features, target, shuffle=True, batch_size=1024):\n",
    "  ds = tf.data.Dataset.from_tensor_slices((features, target))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(features))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "  return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= df_to_dataset(X_train, y_train)\n",
    "valid_data= df_to_dataset(X_val, y_val)\n",
    "test_data= df_to_dataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset element_spec=(TensorSpec(shape=(None, 1), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.float64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training an LSTM model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the time for creating a neural network has finally arrived! First, let's encode our comments using a text vectorizor model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 14 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x000002108FCFC8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "#max tokens=max no of words we will remember.\n",
    "encoder=TextVectorization(max_tokens=2000)\n",
    "\n",
    "#train data is composed of comment and the score but we don't really \n",
    "# #need the score for this encoder. So just use a lambda function to pass in the text.\n",
    "encoder.adapt(train_data.map(lambda comment, score: comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's check our vocabulary. These are just some of the words that have been encoded into vectors: (UNK) represents any unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['', '[UNK]', 'to', 'the', 'i', 'and', 'a', 'they', 'for', 'is',\n",
       "       'of', 'have', 'my', 'virgin', 'it', 'service', 'with', 'was',\n",
       "       'that', 'in', 'you', 'not', 'on', 'me', 'customer', 'this', 'but',\n",
       "       'be', 'as', 'had', 'them', 'no', 'are', 'we', 'so', 'been', 'get',\n",
       "       'when', 'up', 'broadband', 'at', 'media', 'will', 'from', 'their',\n",
       "       'would', 'an', 'all', 'contract', 'if'], dtype='<U15')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab=np.array(encoder.get_vocabulary())\n",
    "vocab[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"text_vectorization_7\" (type TextVectorization).\n\nWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\n\nCall arguments received by layer \"text_vectorization_7\" (type TextVectorization):\n  â€¢ inputs=tf.Tensor(shape=(None, None), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [123], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m Sequential([\n\u001b[0;32m      2\u001b[0m         encoder,\n\u001b[0;32m      3\u001b[0m         Embedding(input_dim\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(encoder\u001b[39m.\u001b[39;49mget_vocabulary()), output_dim\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, mask_zero\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\u001b[39m#mask=0 so we can handle inputs of variable lengths\u001b[39;49;00m\n\u001b[0;32m      4\u001b[0m         \u001b[39m#now we have a vector of numbers a nn can comprehend\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m         LSTM(\u001b[39m32\u001b[39;49m),\n\u001b[0;32m      6\u001b[0m         Dense(\u001b[39m32\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrelu\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m      7\u001b[0m         Dropout(\u001b[39m0.4\u001b[39;49m),\n\u001b[0;32m      8\u001b[0m         Dense(\u001b[39m5\u001b[39;49m, activation\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msoftmax\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      9\u001b[0m ])\n",
      "File \u001b[1;32mc:\\Users\\amilc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\trackable\\base.py:205\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    206\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\amilc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\amilc\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\layers\\preprocessing\\text_vectorization.py:564\u001b[0m, in \u001b[0;36mTextVectorization._preprocess\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[39mif\u001b[39;00m inputs\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    563\u001b[0m     \u001b[39mif\u001b[39;00m inputs\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m--> 564\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    565\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mWhen using `TextVectorization` to tokenize strings, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe input rank must be 1 or the last shape dimension \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    567\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmust be 1. Received: inputs.shape=\u001b[39m\u001b[39m{\u001b[39;00minputs\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwith rank=\u001b[39m\u001b[39m{\u001b[39;00minputs\u001b[39m.\u001b[39mshape\u001b[39m.\u001b[39mrank\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m         )\n\u001b[0;32m    570\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    571\u001b[0m         inputs \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39msqueeze(inputs, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"text_vectorization_7\" (type TextVectorization).\n\nWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\n\nCall arguments received by layer \"text_vectorization_7\" (type TextVectorization):\n  â€¢ inputs=tf.Tensor(shape=(None, None), dtype=string)"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "        encoder,\n",
    "        Embedding(input_dim=len(encoder.get_vocabulary()), output_dim=32, mask_zero=True),#mask=0 so we can handle inputs of variable lengths\n",
    "        #now we have a vector of numbers a nn can comprehend\n",
    "        LSTM(32),\n",
    "        Dense(32, activation=\"relu\"),\n",
    "        Dropout(0.4),\n",
    "        Dense(5, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = [EarlyStopping(monitor='val_loss', patience=5),\n",
    "             ModelCheckpoint(filepath='saved_model', monitor='val_loss', save_best_only=True)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(learning_rate=0.001), \n",
    "             loss = SparseCategoricalCrossentropy(), #categorical cross entropy as multi classification problem\n",
    "                metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 15s 2s/step - loss: 1.6108 - accuracy: 0.1195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.6107749938964844, 0.11949323117733002]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(train_data) #evaluate performance of model without training it first\n",
    "#accuracy is around 0.36.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "4/4 [==============================] - 162s 44s/step - loss: 1.6025 - accuracy: 0.3850 - val_loss: 1.5819 - val_accuracy: 0.6475\n",
      "Epoch 2/50\n",
      "4/4 [==============================] - 146s 31s/step - loss: 1.5703 - accuracy: 0.6432 - val_loss: 1.5421 - val_accuracy: 0.6498\n",
      "Epoch 3/50\n",
      "4/4 [==============================] - 142s 42s/step - loss: 1.5237 - accuracy: 0.6407 - val_loss: 1.4777 - val_accuracy: 0.6498\n",
      "Epoch 4/50\n",
      "4/4 [==============================] - 138s 38s/step - loss: 1.4443 - accuracy: 0.6467 - val_loss: 1.3540 - val_accuracy: 0.6498\n",
      "Epoch 5/50\n",
      "4/4 [==============================] - 105s 28s/step - loss: 1.2894 - accuracy: 0.6421 - val_loss: 1.1037 - val_accuracy: 0.6498\n",
      "Epoch 6/50\n",
      "4/4 [==============================] - 139s 27s/step - loss: 1.0789 - accuracy: 0.6392 - val_loss: 0.9822 - val_accuracy: 0.6498\n",
      "Epoch 7/50\n",
      "4/4 [==============================] - 138s 39s/step - loss: 1.0407 - accuracy: 0.6384 - val_loss: 0.9628 - val_accuracy: 0.6498\n",
      "Epoch 8/50\n",
      "4/4 [==============================] - 139s 39s/step - loss: 0.9984 - accuracy: 0.6358 - val_loss: 0.9308 - val_accuracy: 0.6498\n",
      "Epoch 9/50\n",
      "4/4 [==============================] - 146s 29s/step - loss: 0.9997 - accuracy: 0.6139 - val_loss: 0.9171 - val_accuracy: 0.6498\n",
      "Epoch 10/50\n",
      "4/4 [==============================] - 111s 18s/step - loss: 0.9772 - accuracy: 0.5969 - val_loss: 0.9119 - val_accuracy: 0.6498\n",
      "Epoch 11/50\n",
      "4/4 [==============================] - 155s 46s/step - loss: 0.9695 - accuracy: 0.5811 - val_loss: 0.9042 - val_accuracy: 0.6498\n",
      "Epoch 12/50\n",
      "4/4 [==============================] - 164s 45s/step - loss: 0.9436 - accuracy: 0.6104 - val_loss: 0.8952 - val_accuracy: 0.6498\n",
      "Epoch 13/50\n",
      "4/4 [==============================] - 147s 29s/step - loss: 0.9402 - accuracy: 0.6214 - val_loss: 0.8884 - val_accuracy: 0.6498\n",
      "Epoch 14/50\n",
      "4/4 [==============================] - 167s 48s/step - loss: 0.9323 - accuracy: 0.6265 - val_loss: 0.8835 - val_accuracy: 0.6498\n",
      "Epoch 15/50\n",
      "4/4 [==============================] - 156s 31s/step - loss: 0.9314 - accuracy: 0.6263 - val_loss: 0.8801 - val_accuracy: 0.6498\n",
      "Epoch 16/50\n",
      "4/4 [==============================] - 165s 33s/step - loss: 0.9217 - accuracy: 0.6297 - val_loss: 0.8766 - val_accuracy: 0.6498\n",
      "Epoch 17/50\n",
      "4/4 [==============================] - 129s 35s/step - loss: 0.9186 - accuracy: 0.6309 - val_loss: 0.8734 - val_accuracy: 0.6498\n",
      "Epoch 18/50\n",
      "4/4 [==============================] - 168s 48s/step - loss: 0.9159 - accuracy: 0.6363 - val_loss: 0.8707 - val_accuracy: 0.6498\n",
      "Epoch 19/50\n",
      "4/4 [==============================] - 186s 38s/step - loss: 0.9181 - accuracy: 0.6254 - val_loss: 0.8684 - val_accuracy: 0.6498\n",
      "Epoch 20/50\n",
      "4/4 [==============================] - 140s 22s/step - loss: 0.9094 - accuracy: 0.6337 - val_loss: 0.8665 - val_accuracy: 0.6498\n",
      "Epoch 21/50\n",
      "4/4 [==============================] - 188s 34s/step - loss: 0.9104 - accuracy: 0.6412 - val_loss: 0.8648 - val_accuracy: 0.6498\n",
      "Epoch 22/50\n",
      "4/4 [==============================] - 152s 42s/step - loss: 0.9102 - accuracy: 0.6326 - val_loss: 0.8632 - val_accuracy: 0.6498\n",
      "Epoch 23/50\n",
      "4/4 [==============================] - 150s 40s/step - loss: 0.8971 - accuracy: 0.6363 - val_loss: 0.8617 - val_accuracy: 0.6498\n",
      "Epoch 24/50\n",
      "4/4 [==============================] - 215s 44s/step - loss: 0.9004 - accuracy: 0.6366 - val_loss: 0.8607 - val_accuracy: 0.6498\n",
      "Epoch 25/50\n",
      "4/4 [==============================] - 154s 43s/step - loss: 0.8969 - accuracy: 0.6386 - val_loss: 0.8599 - val_accuracy: 0.6498\n",
      "Epoch 26/50\n",
      "4/4 [==============================] - 157s 42s/step - loss: 0.8930 - accuracy: 0.6450 - val_loss: 0.8589 - val_accuracy: 0.6498\n",
      "Epoch 27/50\n",
      "4/4 [==============================] - 203s 57s/step - loss: 0.8922 - accuracy: 0.6398 - val_loss: 0.8581 - val_accuracy: 0.6498\n",
      "Epoch 28/50\n",
      "4/4 [==============================] - 159s 23s/step - loss: 0.8838 - accuracy: 0.6435 - val_loss: 0.8573 - val_accuracy: 0.6498\n",
      "Epoch 29/50\n",
      "4/4 [==============================] - 185s 51s/step - loss: 0.8879 - accuracy: 0.6415 - val_loss: 0.8566 - val_accuracy: 0.6498\n",
      "Epoch 30/50\n",
      "4/4 [==============================] - 184s 52s/step - loss: 0.8792 - accuracy: 0.6464 - val_loss: 0.8558 - val_accuracy: 0.6498\n",
      "Epoch 31/50\n",
      "4/4 [==============================] - 229s 70s/step - loss: 0.8804 - accuracy: 0.6481 - val_loss: 0.8547 - val_accuracy: 0.6498\n",
      "Epoch 32/50\n",
      "4/4 [==============================] - 269s 54s/step - loss: 0.8758 - accuracy: 0.6476 - val_loss: 0.8539 - val_accuracy: 0.6498\n",
      "Epoch 33/50\n",
      "4/4 [==============================] - 277s 82s/step - loss: 0.8758 - accuracy: 0.6409 - val_loss: 0.8531 - val_accuracy: 0.6498\n",
      "Epoch 34/50\n",
      "4/4 [==============================] - 258s 51s/step - loss: 0.8653 - accuracy: 0.6464 - val_loss: 0.8526 - val_accuracy: 0.6498\n",
      "Epoch 35/50\n",
      "4/4 [==============================] - 251s 46s/step - loss: 0.8614 - accuracy: 0.6481 - val_loss: 0.8516 - val_accuracy: 0.6498\n",
      "Epoch 36/50\n",
      "4/4 [==============================] - 251s 71s/step - loss: 0.8616 - accuracy: 0.6461 - val_loss: 0.8511 - val_accuracy: 0.6498\n",
      "Epoch 37/50\n",
      "4/4 [==============================] - 252s 77s/step - loss: 0.8544 - accuracy: 0.6496 - val_loss: 0.8509 - val_accuracy: 0.6498\n",
      "Epoch 38/50\n",
      "4/4 [==============================] - 257s 48s/step - loss: 0.8415 - accuracy: 0.6504 - val_loss: 0.8505 - val_accuracy: 0.6498\n",
      "Epoch 39/50\n",
      "4/4 [==============================] - 197s 55s/step - loss: 0.8435 - accuracy: 0.6522 - val_loss: 0.8492 - val_accuracy: 0.6498\n",
      "Epoch 40/50\n",
      "4/4 [==============================] - 261s 73s/step - loss: 0.8305 - accuracy: 0.6499 - val_loss: 0.8492 - val_accuracy: 0.6498\n",
      "Epoch 41/50\n",
      "4/4 [==============================] - 257s 72s/step - loss: 0.8202 - accuracy: 0.6542 - val_loss: 0.8509 - val_accuracy: 0.6498\n",
      "Epoch 42/50\n",
      "4/4 [==============================] - 249s 46s/step - loss: 0.8165 - accuracy: 0.6574 - val_loss: 0.8521 - val_accuracy: 0.6498\n",
      "Epoch 43/50\n",
      "4/4 [==============================] - 276s 54s/step - loss: 0.8055 - accuracy: 0.6597 - val_loss: 0.8585 - val_accuracy: 0.6498\n",
      "Epoch 44/50\n",
      "4/4 [==============================] - 261s 48s/step - loss: 0.7876 - accuracy: 0.6669 - val_loss: 0.8693 - val_accuracy: 0.6336\n",
      "Epoch 45/50\n",
      "4/4 [==============================] - 294s 58s/step - loss: 0.7709 - accuracy: 0.6795 - val_loss: 0.8797 - val_accuracy: 0.6313\n",
      "Epoch 46/50\n",
      "4/4 [==============================] - 276s 77s/step - loss: 0.7606 - accuracy: 0.6887 - val_loss: 0.8983 - val_accuracy: 0.6382\n",
      "Epoch 47/50\n",
      "4/4 [==============================] - 192s 24s/step - loss: 0.7374 - accuracy: 0.6942 - val_loss: 0.8981 - val_accuracy: 0.5968\n",
      "Epoch 48/50\n",
      "4/4 [==============================] - 281s 53s/step - loss: 0.7368 - accuracy: 0.6962 - val_loss: 0.9111 - val_accuracy: 0.6060\n",
      "Epoch 49/50\n",
      "4/4 [==============================] - 266s 77s/step - loss: 0.7128 - accuracy: 0.7092 - val_loss: 0.9347 - val_accuracy: 0.6175\n",
      "Epoch 50/50\n",
      "4/4 [==============================] - 271s 76s/step - loss: 0.7056 - accuracy: 0.7118 - val_loss: 0.9497 - val_accuracy: 0.5737\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data, epochs=50, validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3b44d0901b27b90b29836124934e882c64ed863e8408c20b31f73c13ca8ef04"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
